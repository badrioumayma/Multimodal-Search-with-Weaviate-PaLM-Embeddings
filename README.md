# 🧠 Multimodal Search with Weaviate & PaLM Embeddings

This project demonstrates how to build a **multimodal semantic search engine** using:
- Images
- Videos
- Text queries

It uses **Google’s PaLM multimodal embeddings** and stores them in a **Weaviate vector database**, allowing intelligent search across different media types (text-to-image, image-to-video, video-to-media).

---

## 🚀 Features

- 📸 Upload and vectorize **images and videos**
- 🔎 Perform search using:
  - Text queries (e.g., “dog playing with stick”)
  - Input images
  - Input videos
- 🧠 Uses **multi2vec-palm** model for vectorization
- 📊 Visualizes the embedding space using **UMAP**

---

## 🧰 Technologies Used

- Python
- Weaviate
- multi2vec-palm (Google PaLM embedding model)
- Base64 encoding
- UMAP, matplotlib
- Jupyter Notebook


