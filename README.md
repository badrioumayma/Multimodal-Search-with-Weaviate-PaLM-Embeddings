# ğŸ§  Multimodal Search with Weaviate & PaLM Embeddings

This project demonstrates how to build a **multimodal semantic search engine** using:
- Images
- Videos
- Text queries

It uses **Googleâ€™s PaLM multimodal embeddings** and stores them in a **Weaviate vector database**, allowing intelligent search across different media types (text-to-image, image-to-video, video-to-media).

---

## ğŸš€ Features

- ğŸ“¸ Upload and vectorize **images and videos**
- ğŸ” Perform search using:
  - Text queries (e.g., â€œdog playing with stickâ€)
  - Input images
  - Input videos
- ğŸ§  Uses **multi2vec-palm** model for vectorization
- ğŸ“Š Visualizes the embedding space using **UMAP**

---

## ğŸ§° Technologies Used

- Python
- Weaviate
- multi2vec-palm (Google PaLM embedding model)
- Base64 encoding
- UMAP, matplotlib
- Jupyter Notebook


